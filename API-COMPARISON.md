# Pure Functional API - æ–°èˆŠå°æ¯”

## ğŸ¯ æ ¸å¿ƒç†å¿µ

**èˆŠ API (é¡åˆ¥å°å‘):**
- ä½¿ç”¨ `new` é—œéµå­—
- é¡åˆ¥å¯¦ä¾‹
- éš±è—ç‹€æ…‹
- æ–¹æ³•èª¿ç”¨

**æ–° API (ç´”å‡½æ•¸):**
- ç„¡ `new` é—œéµå­—
- å·¥å» å‡½æ•¸
- æ˜ç¢ºç‹€æ…‹å‚³é
- å‡½æ•¸çµ„åˆ

---

## ğŸ“¦ 1. å‰µå»ºæ¨¡å‹

### èˆŠ API
```typescript
import { Linear, Sequential } from '@sylphx/nn'

// ä½¿ç”¨é¡åˆ¥
const model = new Sequential([
  new Linear(2, 8),
  new Tanh(),
  new Linear(8, 1)
])

// ç‹€æ…‹éš±è—åœ¨å¯¦ä¾‹å…§
// ç„¡æ³•ç›´æ¥è¨ªå•æˆ–åºåˆ—åŒ–
```

### æ–° API âœ…
```typescript
import { Sequential, Linear, Tanh } from '@sylphx/nn'

// ä½¿ç”¨å‡½æ•¸çµ„åˆ
const model = Sequential(
  Linear(2, 8),
  Tanh(),
  Linear(8, 1)
)

// æ˜ç¢ºçš„ç‹€æ…‹ç®¡ç†
let modelState = model.init()

// ç‹€æ…‹æ˜¯ç´”æ•¸æ“šï¼Œå¯ä»¥åºåˆ—åŒ–
console.log(modelState)
```

---

## ğŸ”§ 2. å„ªåŒ–å™¨

### èˆŠ API
```typescript
import { Adam } from '@sylphx/optim'

// å‰µå»ºå„ªåŒ–å™¨å¯¦ä¾‹
const optimizer = new Adam(model.parameters(), {
  lr: 0.01,
  beta1: 0.9,
  beta2: 0.999
})

// ç‹€æ…‹éš±è—åœ¨é¡åˆ¥å…§
optimizer.step()
```

### æ–° API âœ…
```typescript
import { Adam } from '@sylphx/optim'
import { getParams } from '@sylphx/train'

// å„ªåŒ–å™¨å·¥å» 
const optimizer = Adam({
  lr: 0.01,
  beta1: 0.9,
  beta2: 0.999
})

// æ˜ç¢ºåˆå§‹åŒ–
let optState = optimizer.init(getParams(modelState))

// æ˜ç¢ºçš„ç‹€æ…‹æ›´æ–°
const result = optimizer.step(params, grads, optState)
optState = result.optState
params = result.params
```

---

## ğŸ“ 3. è¨“ç·´å¾ªç’°

### èˆŠ API
```typescript
// éœ€è¦æ‰‹å‹•ç®¡ç†æ‰€æœ‰ç´°ç¯€
for (let epoch = 0; epoch < 1000; epoch++) {
  // Forward
  const output = model.forward(x)

  // Loss
  const loss = mse(output, y)

  // Backward
  optimizer.zeroGrad()
  loss.backward()

  // Update (ç‹€æ…‹éš±è—)
  optimizer.step()
}
```

### æ–° API âœ…
```typescript
import { trainStep } from '@sylphx/train'

for (let epoch = 0; epoch < 1000; epoch++) {
  const result = trainStep({
    model,
    modelState,
    optimizer,
    optState,
    input: x,
    target: y,
    lossFn: mse
  })

  // æ˜ç¢ºæ›´æ–°ç‹€æ…‹
  modelState = result.modelState
  optState = result.optState

  console.log(`Loss: ${result.loss}`)
}
```

---

## ğŸ—ï¸ 4. DQN Agent (2048 éŠæˆ²)

### èˆŠ API
```typescript
// âŒ 286 è¡Œä»£ç¢¼

// å®šç¾©ç¶²çµ¡çµæ§‹
export type QNetwork = {
  linear1: nn.LinearState
  linear2: nn.LinearState
  linear3: nn.LinearState
}

// æ‰‹å‹•åˆå§‹åŒ–æ¯ä¸€å±¤
export function initNetwork(): QNetwork {
  return {
    linear1: nn.linear.init(16, 64),
    linear2: nn.linear.init(64, 64),
    linear3: nn.linear.init(64, 4),
  }
}

// æ‰‹å‹•å‰å‘å‚³æ’­
export function forward(state: number[], network: QNetwork): Tensor {
  const input = tensor([state], { requiresGrad: false })
  let h = nn.linear.forward(input, network.linear1)
  h = F.relu(h)
  h = nn.linear.forward(h, network.linear2)
  h = F.relu(h)
  h = nn.linear.forward(h, network.linear3)
  return h
}

// æ‰‹å‹•ç²å–åƒæ•¸
function getNetworkParams(network: QNetwork): Tensor[] {
  return [
    network.linear1.weight,
    network.linear1.bias,
    network.linear2.weight,
    network.linear2.bias,
    network.linear3.weight,
    network.linear3.bias,
  ]
}

// æ‰‹å‹•è¨“ç·´æ­¥é©Ÿ
const loss = F.mse(qValuesBatch, target)
const grads = T.backward(loss)
const result = optim.adam.step(agent.optimizer, getNetworkParams(agent.network), grads)

// æ‰‹å‹•é‡å»ºç¶²çµ¡
const newNetwork: QNetwork = {
  linear1: {
    weight: result.params[0]!,
    bias: result.params[1]!,
  },
  linear2: {
    weight: result.params[2]!,
    bias: result.params[3]!,
  },
  linear3: {
    weight: result.params[4]!,
    bias: result.params[5]!,
  },
}
```

### æ–° API âœ…
```typescript
// âœ… 234 è¡Œä»£ç¢¼ (-18%)

import { Sequential, Linear, ReLU } from '@sylphx/nn'
import { Adam } from '@sylphx/optim'
import { getParams, trainStep } from '@sylphx/train'

// ä½¿ç”¨ Sequential çµ„åˆ
const createQNetwork = () => Sequential(
  Linear(16, 64),
  ReLU(),
  Linear(64, 64),
  ReLU(),
  Linear(64, 4)
)

// ç°¡å–®åˆå§‹åŒ–
const model = createQNetwork()
const modelState = model.init()
const optimizer = Adam({ lr: 0.001 })
const optState = optimizer.init(getParams(modelState))

// è‡ªå‹•å‰å‘å‚³æ’­
const qValues = model.forward(input, modelState)

// ä½¿ç”¨ trainStep è‡ªå‹•è™•ç†æ‰€æœ‰ç´°ç¯€
const result = trainStep({
  model,
  modelState,
  optimizer,
  optState,
  input: statesTensor,
  target: target,
  lossFn: F.mse
})

// ç°¡å–®æ›´æ–°
modelState = result.modelState
optState = result.optState
```

---

## ğŸ“Š 5. å®Œæ•´ XOR ä¾‹å­å°æ¯”

### èˆŠ API
```typescript
import { Linear, Sequential, Tanh } from '@sylphx/nn'
import { Adam } from '@sylphx/optim'
import { mse } from '@sylphx/functional'

// æ•¸æ“š
const x = tensor([[0, 0], [0, 1], [1, 0], [1, 1]])
const y = tensor([[0], [1], [1], [0]])

// æ¨¡å‹
const model = new Sequential([
  new Linear(2, 8),
  new Tanh(),
  new Linear(8, 1)
])

// å„ªåŒ–å™¨
const optimizer = new Adam(model.parameters(), { lr: 0.05 })

// è¨“ç·´
for (let epoch = 0; epoch < 3000; epoch++) {
  const output = model.forward(x)
  const loss = mse(output, y)

  optimizer.zeroGrad()
  loss.backward()
  optimizer.step()

  if (epoch % 500 === 0) {
    console.log(`Epoch ${epoch}, Loss: ${loss.item()}`)
  }
}

// æ¸¬è©¦
const pred = model.forward(tensor([[0, 1]]))
console.log(pred.item())
```

### æ–° API âœ…
```typescript
import { tensor } from '@sylphx/tensor'
import { Sequential, Linear, Tanh } from '@sylphx/nn'
import { Adam } from '@sylphx/optim'
import { mse } from '@sylphx/functional'
import { getParams, trainStep } from '@sylphx/train'

// æ•¸æ“š
const x = tensor([[0, 0], [0, 1], [1, 0], [1, 1]], { requiresGrad: true })
const y = tensor([[0], [1], [1], [0]], { requiresGrad: true })

// æ¨¡å‹
const model = Sequential(
  Linear(2, 8),
  Tanh(),
  Linear(8, 1)
)

// åˆå§‹åŒ–
let modelState = model.init()
const optimizer = Adam({ lr: 0.05 })
let optState = optimizer.init(getParams(modelState))

// è¨“ç·´
for (let epoch = 0; epoch < 3000; epoch++) {
  const result = trainStep({
    model,
    modelState,
    optimizer,
    optState,
    input: x,
    target: y,
    lossFn: mse
  })

  modelState = result.modelState
  optState = result.optState

  if (epoch % 500 === 0) {
    console.log(`Epoch ${epoch}, Loss: ${result.loss}`)
  }
}

// æ¸¬è©¦
const pred = model.forward(tensor([[0, 1]]), modelState)
console.log(pred.data[0])
```

---

## ğŸ¯ é—œéµå·®ç•°ç¸½çµ

| ç‰¹æ€§ | èˆŠ API | æ–° API |
|------|--------|--------|
| **é¡åˆ¥** | âœ… ä½¿ç”¨ `new` | âŒ ç„¡é¡åˆ¥ |
| **ç‹€æ…‹** | éš±è— | æ˜ç¢º |
| **çµ„åˆ** | æ•¸çµ„ `[...]` | å‡½æ•¸ `Sequential(...)` |
| **ä¸è®Šæ€§** | âŒ å¯è®Š | âœ… ä¸å¯è®Š |
| **åºåˆ—åŒ–** | å›°é›£ | ç°¡å–® |
| **æ¸¬è©¦** | å›°é›£ (å‰¯ä½œç”¨) | ç°¡å–® (ç´”å‡½æ•¸) |
| **ä»£ç¢¼é‡** | æ›´å¤š | æ›´å°‘ (-18%) |
| **å¯è®€æ€§** | ä¸­ç­‰ | é«˜ |

---

## âœ¨ æ–° API å„ªå‹¢

1. **ç´”å‡½æ•¸** - ç„¡å‰¯ä½œç”¨ï¼Œæ˜“æ¸¬è©¦
2. **æ˜ç¢ºç‹€æ…‹** - ç‹€æ…‹å¯è¦‹ã€å¯æ§ã€å¯åºåˆ—åŒ–
3. **ä¸å¯è®Š** - å‡½æ•¸å¼ç·¨ç¨‹æœ€ä½³å¯¦è¸
4. **çµ„åˆæ€§** - ä½¿ç”¨ Sequential çµ„åˆå±¤
5. **ç°¡æ½”** - trainStep è‡ªå‹•è™•ç†ç´°ç¯€
6. **é¡å‹å®‰å…¨** - TypeScript å®Œæ•´æ”¯æ´
7. **æ˜“ç†è§£** - æ•¸æ“šæµå‘æ¸…æ™°

---

## ğŸš€ é·ç§»å»ºè­°

èˆŠä»£ç¢¼å·²ç¶“ä¸æ”¯æ´ï¼æ‰€æœ‰é …ç›®å¿…é ˆé·ç§»åˆ°æ–° APIã€‚

**æ­¥é©Ÿ:**
1. ç§»é™¤æ‰€æœ‰ `new` é—œéµå­—
2. ä½¿ç”¨å·¥å» å‡½æ•¸: `Linear(2, 8)` ä»£æ›¿ `new Linear(2, 8)`
3. æ˜ç¢ºç®¡ç†ç‹€æ…‹: `modelState`, `optState`
4. ä½¿ç”¨ `trainStep` ç°¡åŒ–è¨“ç·´
5. ä½¿ç”¨ `Sequential` çµ„åˆå±¤

**æ”¶ç›Š:**
- ä»£ç¢¼æ¸›å°‘ ~20%
- å¯è®€æ€§æå‡
- æ›´æ˜“ç¶­è­·
- å®Œå…¨é¡å‹å®‰å…¨
